from pathlib import Path
import re
import shutil
from typing import List, Tuple, Dict, Any, Optional

from bs4 import BeautifulSoup
# â–¼â–¼â–¼ å¤‰æ›´ â–¼â–¼â–¼
from langchain_core.prompts import BasePromptTemplate, ChatPromptTemplate
# â–²â–²â–² å¤‰æ›´ â–²â–²â–²
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_experimental.graph_transformers import LLMGraphTransformer
import config
from langchain_core.documents import Document
from langchain_neo4j import Neo4jGraph
from neo4j.exceptions import ServiceUnavailable
from langchain_community.graphs.graph_document import GraphDocument
from langchain_community.vectorstores import Chroma


# --- å®šæ•°å®šç¾© ---
DATA_DIR = Path("data")
NEO4J_URI = config.NEO4J_URI
NEO4J_USER = config.NEO4J_USER
NEO4J_PASSWORD = config.NEO4J_PASSWORD
NEO4J_DATABASE = getattr(config, "NEO4J_DATABASE", "neo4j")

API_TXT_CANDIDATES = [
    Path("/mnt/data/api.txt"),
    Path("api.txt"),
    DATA_DIR / "api.txt",
]

API_ARG_TXT_CANDIDATES = [
    Path("/mnt/data/api_arg.txt"),
    Path("api_arg.txt"),
    DATA_DIR / "api_arg.txt",
]

CHROMA_PERSIST_DIR = DATA_DIR / "chroma_db"
OPENAI_API_KEY = config.OPENAI_API_KEY

# â–¼â–¼â–¼ å¤‰æ›´ â–¼â–¼â–¼
# --- LLMGraphTransformerç”¨ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾© ---
# ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãªã„å ´åˆã¯ã€ã“ã®å€¤ã‚’ None ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚
# None ã«è¨­å®šã™ã‚‹ã¨ã€LLMGraphTransformerã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
CUSTOM_GRAPH_TRANSFORMER_PROMPT: Optional[BasePromptTemplate] = None

# --- ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨˜è¿°ä¾‹ ---
# ä»¥ä¸‹ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€CUSTOM_GRAPH_TRANSFORMER_PROMPT ã«ä»£å…¥ã—ã¦ãã ã•ã„ã€‚
# ä¾‹: CUSTOM_GRAPH_TRANSFORMER_PROMPT = custom_prompt_example

custom_prompt_example = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            # Role: çŸ¥è­˜ã‚°ãƒ©ãƒ•æŠ½å‡ºã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ
            ã‚ãªãŸã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ã‚’æŠ½å‡ºã—ã€çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
            æä¾›ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦ã€ä¸»è¦ãªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ãã‚Œã‚‰ã®é–“ã®é–¢é€£æ€§ã‚’ç‰¹å®šã™ã‚‹ã“ã¨ãŒã‚ãªãŸã®å½¹å‰²ã§ã™ã€‚

            ## æŒ‡ç¤º
            1.  **ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ç‰¹å®š**: ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¸»è¦ãªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆãƒãƒ¼ãƒ‰ï¼‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«ã¯ä¸€æ„ã®`id`ã¨`type`ï¼ˆç¨®é¡ï¼‰ãŒå¿…è¦ã§ã™ã€‚
            2.  **ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ã®ç‰¹å®š**: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é–¢ä¿‚æ€§ï¼ˆã‚¨ãƒƒã‚¸ï¼‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚å„ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ã«ã¯`source`ï¼ˆå§‹ç‚¹ãƒãƒ¼ãƒ‰ï¼‰ã€`target`ï¼ˆçµ‚ç‚¹ãƒãƒ¼ãƒ‰ï¼‰ã€`type`ï¼ˆé–¢ä¿‚æ€§ã®ç¨®é¡ï¼‰ãŒå¿…è¦ã§ã™ã€‚
            3.  **å³å¯†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯ "nodes" ã¨ "relationships" ã‚’å«ã‚€JSONå½¢å¼ã®ã¿ã§è¿”å´ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã‚„è¬ç½ªãªã©ã®è¿½åŠ ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚

            ## ä¾‹
            ãƒ†ã‚­ã‚¹ãƒˆ: "Aç¤¾ã®ç”°ä¸­ã¯ã€Bç¤¾ã®éˆ´æœ¨ã¨å”åŠ›ã—ã¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆXã‚’é€²ã‚ã¦ã„ã‚‹ã€‚ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯æ±äº¬ã§è¡Œã‚ã‚Œã¦ã„ã‚‹ã€‚"
            JSON:
            {
                "nodes": [
                    {"id": "ç”°ä¸­", "type": "Person"},
                    {"id": "Aç¤¾", "type": "Company"},
                    {"id": "éˆ´æœ¨", "type": "Person"},
                    {"id": "Bç¤¾", "type": "Company"},
                    {"id": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆX", "type": "Project"},
                    {"id": "æ±äº¬", "type": "Location"}
                ],
                "relationships": [
                    {"source": "ç”°ä¸­", "target": "Aç¤¾", "type": "BELONGS_TO"},
                    {"source": "éˆ´æœ¨", "target": "Bç¤¾", "type": "BELONGS_TO"},
                    {"source": "ç”°ä¸­", "target": "éˆ´æœ¨", "type": "COOPERATES_WITH"},
                    {"source": "ç”°ä¸­", "target": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆX", "type": "WORKS_ON"},
                    {"source": "éˆ´æœ¨", "target": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆX", "type": "WORKS_ON"},
                    {"source": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆX", "target": "æ±äº¬", "type": "LOCATED_IN"}
                ]
            }
            """,
        ),
        (
            "human",
            "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„:\n-----\n{text}",
        ),
    ]
)
# â–²â–²â–² å¤‰æ›´ â–²â–²â–²


# --- ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ»ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–é–¢æ•° (å¤‰æ›´ãªã—) ---

def _read_api_text() -> str:
    """api.txt ã‚’å€™è£œãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã‚€"""
    for p in API_TXT_CANDIDATES:
        if p.exists():
            return p.read_text(encoding="utf-8")
    raise FileNotFoundError("api.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

def _read_api_arg_text() -> str:
    """api_arg.txt ã‚’å€™è£œãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã‚€"""
    for p in API_ARG_TXT_CANDIDATES:
        if p.exists():
            return p.read_text(encoding="utf-8")
    raise FileNotFoundError("api_arg.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

def _read_script_files() -> List[Tuple[str, str]]:
    """data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã® .py ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦èª­ã¿è¾¼ã‚€"""
    script_files = []
    if not DATA_DIR.exists():
        return []
    
    for p in DATA_DIR.glob("*.py"):
        if p.is_file():
            script_files.append((p.name, p.read_text(encoding="utf-8")))
            
    return script_files

def _read_html_files() -> List[Tuple[str, str]]:
    """data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã® .html ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦èª­ã¿è¾¼ã‚€"""
    html_files = []
    if not DATA_DIR.exists():
        return []
    
    for p in DATA_DIR.glob("*.html"):
        if p.is_file():
            try:
                content = p.read_text(encoding="shift_jis")
            except UnicodeDecodeError:
                content = p.read_text(encoding="utf-8")
            html_files.append((p.name, content))
            
    return html_files

def _normalize_text(text: str) -> str:
    """æ”¹è¡Œ/ã‚¿ãƒ–/ç©ºç™½ã®æºã‚Œã‚’æ­£è¦åŒ–ã€‚"""
    text = text.replace("\ufeff", "")  # BOM
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = "\n".join(line.rstrip() for line in text.split("\n"))
    text = text.replace("\t", " ")
    text = re.sub(r"[ \u00A0\u3000]+", " ", text)
    return text


# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œé–¢æ•° (å¤‰æ›´ãªã—) ---

def _rebuild_graph_in_neo4j(graph_docs: List[GraphDocument]) -> Tuple[int, int]:
    """Neo4j ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã‹ã‚‰ GraphDocument ã‚’æŠ•å…¥ã™ã‚‹"""
    graph = Neo4jGraph(
        url=NEO4J_URI,
        username=NEO4J_USER,
        password=NEO4J_PASSWORD,
        database=NEO4J_DATABASE,
    )
    
    print("ğŸ§¹ Neo4jã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ä¸­...")
    delete_query = "MATCH (n) DETACH DELETE n"
    graph.query(delete_query)
    
    print(f"\nğŸš€ Neo4jã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ä¸­...")
    graph.add_graph_documents(graph_docs)
    
    res_nodes = graph.query("MATCH (n) RETURN count(n) AS c")
    res_rels = graph.query("MATCH ()-[r]->() RETURN count(r) AS c")
    return int(res_nodes[0]["c"]), int(res_rels[0]["c"])

def _build_and_load_chroma(docs_for_vectorstore: List[Document]) -> None:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«DB (Chroma) ã‚’æ§‹ç¯‰ãƒ»æ°¸ç¶šåŒ–ã™ã‚‹"""
    print("\nğŸš€ ChromaDBã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆãƒ»ä¿å­˜ä¸­...")

    if CHROMA_PERSIST_DIR.exists():
        shutil.rmtree(CHROMA_PERSIST_DIR)
    CHROMA_PERSIST_DIR.mkdir(exist_ok=True)

    try:
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        vectorstore = Chroma.from_documents(
            documents=docs_for_vectorstore,
            embedding=embeddings,
            persist_directory=str(CHROMA_PERSIST_DIR),
        )
        print(
            f"âœ” Chroma DB created and persisted with {len(docs_for_vectorstore)} documents at: {CHROMA_PERSIST_DIR}"
        )
    except Exception as e:
        print(f"âš  Chroma DBã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


# --- ãƒ¡ã‚¤ãƒ³ã®ã‚°ãƒ©ãƒ•æ§‹ç¯‰ãƒ»ãƒ­ãƒ¼ãƒ‰é–¢æ•° ---

def _build_and_load_neo4j() -> None:
    """LLMGraphTransformerã‚’ä½¿ç”¨ã—ã¦ã™ã¹ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã—ã€Neo4jã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    
    # --- 1. ã™ã¹ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ› ---
    print("ğŸ“„ APIä»•æ§˜æ›¸ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã€HTMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
    documents = []
    
    try:
        documents.append(Document(page_content=_normalize_text(_read_api_text()), metadata={"source": "api.txt"}))
        documents.append(Document(page_content=_normalize_text(_read_api_arg_text()), metadata={"source": "api_arg.txt"}))
    except FileNotFoundError as e:
        print(f"âš  {e}")
        
    for script_name, script_content in _read_script_files():
        documents.append(Document(page_content=script_content, metadata={"source": "script_example", "file_name": script_name}))
        
    for file_name, html_content in _read_html_files():
        soup = BeautifulSoup(html_content, 'lxml')
        content_div = soup.find('div', class_='contents')
        text_content = content_div.get_text(separator='\n', strip=True) if content_div else (soup.body.get_text(separator='\n', strip=True) if soup.body else "")
        documents.append(Document(page_content=text_content, metadata={"source": "html_document", "file_name": file_name}))
        
    if not documents:
        print("âš  å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    print(f"âœ” åˆè¨ˆ {len(documents)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†å¯¾è±¡ã¨ã—ã¾ã™ã€‚")

    # --- 2. LLMã«ã‚ˆã‚‹ã‚°ãƒ©ãƒ•æŠ½å‡º ---
    print("\nğŸ¤– LLMã‚’ä½¿ç”¨ã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­...")
    llm = ChatOpenAI(temperature=0, model_name="gpt-5", openai_api_key=OPENAI_API_KEY)

    # â–¼â–¼â–¼ å¤‰æ›´ â–¼â–¼â–¼
    # CUSTOM_GRAPH_TRANSFORMER_PROMPT ãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã—ã€
    # è¨­å®šã•ã‚Œã¦ã„ãªã‘ã‚Œã° (Noneã®å ´åˆ) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    if CUSTOM_GRAPH_TRANSFORMER_PROMPT:
        print("  - ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ã‚°ãƒ©ãƒ•ã‚’æŠ½å‡ºã—ã¾ã™ã€‚")
        llm_transformer = LLMGraphTransformer(
            llm=llm, prompt=CUSTOM_GRAPH_TRANSFORMER_PROMPT
        )
    else:
        print("  - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ã‚°ãƒ©ãƒ•ã‚’æŠ½å‡ºã—ã¾ã™ã€‚")
        llm_transformer = LLMGraphTransformer(llm=llm)
    # â–²â–²â–² å¤‰æ›´ â–²â–²â–²

    try:
        gdocs = llm_transformer.convert_to_graph_documents(documents)
        print(f"âœ” LLMã«ã‚ˆã‚‹ã‚°ãƒ©ãƒ•æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âš  LLMã«ã‚ˆã‚‹ã‚°ãƒ©ãƒ•æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return

    # --- 3. Neo4jã¸ã®ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ ---
    try:
        node_count, rel_count = _rebuild_graph_in_neo4j(gdocs)
        print(f"âœ” ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å†æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸ: ãƒãƒ¼ãƒ‰={node_count}, ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—={rel_count}")
    except ServiceUnavailable as se:
        print(f"âš  Neo4j ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {se}\n   Neo4jã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ã€æ¥ç¶šæƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        print(f"âš  ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    
    # 1. ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ (Neo4j) ã‚’æ§‹ç¯‰
    # ã“ã®é–¢æ•°å†…ã§å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãŒLLMã«ã‚ˆã£ã¦ã‚°ãƒ©ãƒ•åŒ–ã•ã‚Œã‚‹
    _build_and_load_neo4j()

    # 2. ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ (Chroma) ã‚’æ§‹ç¯‰
    print("\n--- ChromaDBæ§‹ç¯‰ãƒ—ãƒ­ã‚»ã‚¹ ---")
    docs_for_vectorstore: List[Document] = []
    
    # APIä»•æ§˜æ›¸
    try:
        docs_for_vectorstore.append(Document(page_content=_normalize_text(_read_api_text()), metadata={"source": "api_spec"}))
    except FileNotFoundError:
        pass # Neo4jæ§‹ç¯‰æ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã„ã‚‹ã¯ãšãªã®ã§ã“ã“ã§ã¯ç„¡è¦–
        
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹
    script_files = _read_script_files()
    for script_name, script_content in script_files:
        docs_for_vectorstore.append(Document(page_content=f"ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹: {script_name}\n\n```python\n{script_content}\n```", metadata={"source": "script_example", "script_name": script_name}))
    
    # HTMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    for file_name, html_content in _read_html_files():
        soup = BeautifulSoup(html_content, 'lxml')
        content_div = soup.find('div', class_='contents')
        text_content = content_div.get_text(separator='\n', strip=True) if content_div else (soup.body.get_text(separator='\n', strip=True) if soup.body else "")
        docs_for_vectorstore.append(Document(page_content=f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {file_name}\n\n{text_content}", metadata={"source": "html_document", "file_name": file_name}))

    if docs_for_vectorstore:
        print(f"âœ” ChromaDBç”¨ã« {len(docs_for_vectorstore)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æº–å‚™ã—ã¾ã—ãŸã€‚")
        _build_and_load_chroma(docs_for_vectorstore)
    else:
        print("âš  ChromaDBã«å…¥ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒãªã„ãŸã‚ã€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")


if __name__ == "__main__":
    main()