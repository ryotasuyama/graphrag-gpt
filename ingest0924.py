from tree_sitter import Language, Parser
import tree_sitter_python as tspython

from pathlib import Path
import re
import json
from typing import List, Dict, Any, Optional, Tuple
import shutil
# â˜…â˜…â˜… å¤‰æ›´ç‚¹ â˜…â˜…â˜…
from datetime import datetime # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ãŸã‚ã«datetimeã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

from bs4 import BeautifulSoup
from langchain_openai import ChatOpenAI
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_core.prompts import ChatPromptTemplate

import config, logging
from langchain_core.documents import Document
from langchain_neo4j import Neo4jGraph
from neo4j.exceptions import ServiceUnavailable
from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

DATA_DIR = Path("data")
NEO4J_URI = config.NEO4J_URI
NEO4J_USER = config.NEO4J_USER
NEO4J_PASSWORD = config.NEO4J_PASSWORD
NEO4J_DATABASE = getattr(config, "NEO4J_DATABASE", "neo4j")

API_TXT_CANDIDATES = [
    Path("/mnt/data/api.txt"),
    Path("api.txt"),
    DATA_DIR / "api.txt",
]

API_ARG_TXT_CANDIDATES = [
    Path("/mnt/data/api_arg.txt"),
    Path("api_arg.txt"),
    DATA_DIR / "api_arg.txt",
]

# tree-sitterã®Pythonç”¨ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
PY_LANGUAGE = Language(tspython.language())
parser = Parser(PY_LANGUAGE)


CHROMA_PERSIST_DIR = DATA_DIR / "chroma_db"
OPENAI_API_KEY = config.OPENAI_API_KEY

# LLMã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å®šç¾© (HTMLè§£æã¨ã‚³ãƒ¼ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã®ç›®çš„æŠ½å‡ºã§ä½¿ç”¨)
llm = ChatOpenAI(temperature=1, model_name="gpt-5", openai_api_key=OPENAI_API_KEY)


def _split_script_into_chunks(script_content: str) -> List[str]:
    """
    ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é€£ç¶šã™ã‚‹2ã¤ä»¥ä¸Šã®æ”¹è¡Œã§åˆ†å‰²ã—ã€ã‚³ãƒ¼ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    chunks = re.split(r'\n\s*\n', script_content.strip())
    return [chunk.strip() for chunk in chunks if chunk.strip()]

def _get_chunk_purpose(chunk_content: str) -> str:
    """LLMã‚’ä½¿ã£ã¦ã‚³ãƒ¼ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã®ç›®çš„ã‚’ç”Ÿæˆã™ã‚‹"""
    prompt = f"""
    ä»¥ä¸‹ã®Pythonã‚³ãƒ¼ãƒ‰ã®æ–­ç‰‡ãŒã€APIã‚’å‘¼ã³å‡ºã—ã¦ä½•ã‚’è¡ŒãŠã†ã¨ã—ã¦ã„ã‚‹ã®ã‹ã€ãã®ç›®çš„ã‚’ç°¡æ½”ãªæ—¥æœ¬èªã®ä¸€æ–‡ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

    ```python
    {chunk_content}
    ```
    ã“ã®ã‚³ãƒ¼ãƒ‰ã®ç›®çš„:
    """
    try:
        response = llm.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        print(f"      âš  ã‚³ãƒ¼ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã®ç›®çš„ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return "ç›®çš„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

def extract_triples_from_script(
    script_path: str, script_text: str
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ãƒãƒ¼ãƒ‰/ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒˆãƒªãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹"""
    triples: List[Dict[str, Any]] = []
    node_props: Dict[str, Dict[str, Any]] = {}

    script_node_id = script_path
    node_props[script_node_id] = {
        "type": "ScriptExample",
        "properties": {
            "name": script_path,
            "code": script_text
        }
    }

    chunks = _split_script_into_chunks(script_text)
    
    all_methods_in_script = set()

    for i, chunk_text in enumerate(chunks):
        print(f"      - ãƒãƒ£ãƒ³ã‚¯ {i+1}/{len(chunks)} ã®ç›®çš„ã‚’æŠ½å‡ºä¸­...")
        purpose = _get_chunk_purpose(chunk_text)

        chunk_node_id = f"{script_path}_chunk_{i}"
        node_props[chunk_node_id] = {
            "type": "CodeChunk",
            "properties": {"purpose": purpose, "code": chunk_text, "order": i}
        }
        triples.append({
            "source": script_node_id, "source_type": "ScriptExample",
            "label": "HAS_CHUNK", "target": chunk_node_id, "target_type": "CodeChunk"
        })

        method_calls_in_chunk = _extract_method_calls_from_script(chunk_text)
        prev_call_node_id_in_chunk = None

        for j, call in enumerate(method_calls_in_chunk):
            method_name = call["method_name"]
            all_methods_in_script.add(method_name)
            
            call_node_id = f"{script_path}_chunk_{i}_call_{j}"
            node_props[call_node_id] = {
                "type": "MethodCall",
                "properties": {"code": call["full_text"], "order": j}
            }
            
            triples.append({
                "source": chunk_node_id, "source_type": "CodeChunk",
                "label": "CONTAINS", "target": call_node_id, "target_type": "MethodCall"
            })
            
            triples.append({
                "source": call_node_id, "source_type": "MethodCall",
                "label": "CALLS", "target": method_name, "target_type": "Method"
            })

            if prev_call_node_id_in_chunk:
                triples.append({
                    "source": prev_call_node_id_in_chunk, "source_type": "MethodCall",
                    "label": "NEXT", "target": call_node_id, "target_type": "MethodCall"
                })
            
            prev_call_node_id_in_chunk = call_node_id

    for method_name in all_methods_in_script:
        triples.append({
            "source": script_node_id, "source_type": "ScriptExample",
            "label": "IS_EXAMPLE_OF", "target": method_name, "target_type": "Method"
        })

    return triples, node_props

def _read_api_text() -> str:
    """api.txt ã‚’å€™è£œãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã‚€"""
    for p in API_TXT_CANDIDATES:
        if p.exists():
            return p.read_text(encoding="utf-8")
    raise FileNotFoundError("api.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚/mnt/data/api.txt ã¾ãŸã¯ ./api.txt ã‚’ç”¨æ„ã—ã¦ãã ã•ã„ã€‚")

def _read_api_arg_text() -> str:
    """api_arg.txt ã‚’å€™è£œãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã‚€"""
    for p in API_ARG_TXT_CANDIDATES:
        if p.exists():
            return p.read_text(encoding="utf-8")
    raise FileNotFoundError("api_arg.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

def _read_script_files() -> List[Tuple[str, str]]:
    """data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã® .py ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦èª­ã¿è¾¼ã‚€"""
    script_files = []
    if not DATA_DIR.exists():
        return []
    
    for p in DATA_DIR.glob("*.py"):
        if p.is_file():
            script_files.append((p.name, p.read_text(encoding="utf-8")))
            
    return script_files

def _read_html_files() -> List[Tuple[str, str]]:
    """data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã® .html ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦èª­ã¿è¾¼ã‚€"""
    html_files = []
    if not DATA_DIR.exists():
        return []
    
    for p in DATA_DIR.glob("*.html"):
        if p.is_file():
            try:
                content = p.read_text(encoding="shift_jis")
            except UnicodeDecodeError:
                content = p.read_text(encoding="utf-8")
            html_files.append((p.name, content))
            
    return html_files

def _extract_graph_from_html(
    file_name: str, html_content: str
    ) -> List[GraphDocument]:
    """
    LLMGraphTransformer ã‚’ä½¿ç”¨ã—ã¦HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ã‚°ãƒ©ãƒ•ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚
    ã‚¹ã‚­ãƒ¼ãƒã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«ã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®šã€‚
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                ã‚ãªãŸã¯ã€æ—¢å­˜ã®APIçŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’è£œå¼·ã™ã‚‹ãŸã‚ã«ã€æŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
                ç§ãŸã¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã¯ã€APIä»•æ§˜æ›¸ã‹ã‚‰ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ç”Ÿæˆã•ã‚ŒãŸã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ¼ãƒã‚’æŒã¤ã‚°ãƒ©ãƒ•ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚
                - **ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«**: `Object`, `Method`, `Parameter`, `DataType`, `ScriptExample`, `MethodCall`
                - **ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—**: `BELONGS_TO`, `HAS_PARAMETER`, `CALLS`, `IS_EXAMPLE_OF` ãªã©

                ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯ã€æä¾›ã•ã‚ŒãŸHTMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è§£ãã€ã“ã®æ—¢å­˜ã‚°ãƒ©ãƒ•ã‚’è£œå®Œã™ã‚‹ãŸã‚ã®**æ¦‚å¿µ**ã€**æ©Ÿèƒ½èª¬æ˜**ã€**åˆ¶ç´„äº‹é …**ãªã©ã‚’æŠ½å‡ºã—ã€æ—¢å­˜ã®ãƒãƒ¼ãƒ‰ã«æ¥ç¶šã™ã‚‹ã“ã¨ã§ã™ã€‚

                ä»¥ä¸‹ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å³å®ˆã—ã¦ãã ã•ã„ã€‚

                1.  **æ—¢å­˜ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¸ã®ãƒªãƒ³ã‚¯**:
                    - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã€æ˜ã‚‰ã‹ã«æ—¢å­˜ã®APIãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆä¾‹: `CreatePlate`ï¼‰ã‚„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆä¾‹: `Part`ï¼‰ã«ã¤ã„ã¦è¨€åŠã—ã¦ã„ã‚‹å ´åˆã€**æ–°ã—ã„ãƒãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«ä½œã‚‰ãªã„ã§ãã ã•ã„**ã€‚
                    - ä»£ã‚ã‚Šã«ã€ãã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ `id` ã¨ `type`ï¼ˆä¾‹: {{"id": "CreatePlate", "type": "Method"}}ï¼‰ã§æ­£ç¢ºã«æŒ‡å®šã—ã€å¾Œè¿°ã™ã‚‹æ–°ã—ã„`Concept`ãƒãƒ¼ãƒ‰ã¨æ¥ç¶šã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ—¢å­˜ã®ãƒãƒ¼ãƒ‰ã«æ–°ã—ã„æƒ…å ±ãŒãƒªãƒ³ã‚¯ã•ã‚Œã¾ã™ã€‚

                2.  **æ–°ã—ã„æƒ…å ±ã®æŠ½å‡ºï¼ˆConceptãƒãƒ¼ãƒ‰ï¼‰**:
                    - APIã®**æ©Ÿèƒ½**ã€**ã‚³ãƒ³ã‚»ãƒ—ãƒˆ**ï¼ˆä¾‹ï¼šã€Œå‰›ä½“ã®ä½œæˆã€ã€ã€Œåº§æ¨™ç³»ã€ï¼‰ã€**ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**ã€**åˆ¶ç´„äº‹é …**ãªã©ã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ã¯æŠ½å‡ºã§ããªã„è£œè¶³çš„ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                    - ã“ã‚Œã‚‰ã®æ–°ã—ã„æƒ…å ±ã¯ã€å¿…ãš `Concept` ã¨ã„ã†ãƒ©ãƒ™ãƒ«ï¼ˆ`type`ï¼‰ã‚’æŒã¤ãƒãƒ¼ãƒ‰ã¨ã—ã¦è¡¨ç¾ã—ã¾ã™ã€‚
                    - `Concept` ãƒãƒ¼ãƒ‰ã® `id` ã«ã¯ã€ãã®æ¦‚å¿µã‚’ç°¡æ½”ã«è¡¨ã™åç§°ï¼ˆä¾‹ï¼šã€Œãƒ—ãƒ¬ãƒ¼ãƒˆã®åšã¿è¨­å®šã€ï¼‰ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

                3.  **ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å®šç¾©**:
                    - æŠ½å‡ºã—ãŸãƒãƒ¼ãƒ‰é–“ã®é–¢ä¿‚æ€§ã¯ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®ã‚¿ã‚¤ãƒ—ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚
                      - **EXPLAINS** (èª¬æ˜ã™ã‚‹): ã‚ã‚‹`Concept`ãŒã€ç‰¹å®šã®`Method`ã‚„`Object`ã®æ©Ÿèƒ½ãƒ»ä½¿ã„æ–¹ã‚’èª¬æ˜ã—ã¦ã„ã‚‹å ´åˆã€‚
                        (ä¾‹: `(Concept:ãƒ—ãƒ¬ãƒ¼ãƒˆã®åšã¿è¨­å®š) -[:EXPLAINS]-> (Method:CreatePlate)`)
                      - **RELATES_TO** (é–¢é€£ã™ã‚‹): 2ã¤ã®ãƒãƒ¼ãƒ‰ãŒé–¢é€£ã—ã¦ã„ã‚‹ãŒã€ç›´æ¥çš„ãªèª¬æ˜é–¢ä¿‚ã§ã¯ãªã„å ´åˆã€‚
                        (ä¾‹: `(Method:CreatePlate) -[:RELATES_TO]-> (Concept:åº§æ¨™ç³»)`)
                      - **HAS_CONSTRAINT** (åˆ¶ç´„ã‚’æŒã¤): ã‚ã‚‹`Method`ã‚„`Object`ã«ç‰¹å®šã®åˆ¶ç´„äº‹é …ï¼ˆ`Concept`ï¼‰ãŒã‚ã‚‹å ´åˆã€‚
                        (ä¾‹: `(Method:GetSolidID) -[:HAS_CONSTRAINT]-> (Concept:äº‹å‰ã«ã‚½ãƒªãƒƒãƒ‰ã®é¸æŠãŒå¿…è¦)`)

                4.  **å‡ºåŠ›**:
                    - æŒ‡ç¤ºã•ã‚ŒãŸå½¢å¼ï¼ˆãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã¨ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ãƒªã‚¹ãƒˆï¼‰ã§ã€æŠ½å‡ºã—ãŸæƒ…å ±ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä½™åˆ†ãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚
                """,
            ),
            (
                "human",
                "ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã€ä¸Šè¨˜ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«å¾“ã£ã¦çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„:\n\n{input}",
            ),
        ]
    )

    llm_transformer = LLMGraphTransformer(llm=llm, prompt=prompt)

    soup = BeautifulSoup(html_content, 'lxml')
    
    content_div = soup.find('div', class_='contents')
    if content_div:
        text_content = content_div.get_text(separator='\n', strip=True)
    else:
        text_content = soup.body.get_text(separator='\n', strip=True) if soup.body else ""

    doc = Document(
        page_content=text_content,
        metadata={"source": "html_document", "file_name": file_name}
    )
    
    return llm_transformer.convert_to_graph_documents([doc])


def _normalize_text(text: str) -> str:
    text = text.replace("\ufeff", "")
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = "\n".join(line.rstrip() for line in text.split("\n"))
    text = text.replace("\t", " ")
    text = re.sub(r"[ \u00A0\u3000]+", " ", text)
    return text

def _to_object_id_from_header(header: str) -> str:
    s = header.strip()
    s = re.sub(r"^â– ", "", s)
    s = s.replace("ã®ãƒ¡ã‚½ãƒƒãƒ‰", "")
    s = s.replace("ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ", "")
    return s.strip()

def _guess_return_type_from_desc(desc: str) -> str:
    d = desc or ""
    if re.search(r"\bID\b", d, flags=re.IGNORECASE) or ("è¦ç´ ID" in d):
        return "ID"
    return "ä¸æ˜"

def _parse_api_specs(text: str) -> List[Dict[str, Any]]:
    lines = text.split("\n")
    closing_pat = re.compile(r"\)\s*;?(?:\s*//.*)?$")
    param_pat = re.compile(
        r"^([A-Za-z_][A-Za-z0-9_]*)\s*,?\s*//\s*([^:ï¼š]+)\s*[:ï¼š]\s*(.*)$"
    )
    method_start_pat = re.compile(r"^([A-Za-z_][A-Za-z0-9_]*)\s*\($")
    header_pat = re.compile(r"^â– .+ã®ãƒ¡ã‚½ãƒƒãƒ‰$")
    title_pat = re.compile(r"^ã€‡(.+)$")
    ret_pat = re.compile(r"^è¿”ã‚Šå€¤[:ï¼š]\s*(.+)$")

    current_object = None
    current_title = None
    current_return_desc = None
    collecting_params = False
    current_entry: Optional[Dict[str, Any]] = None
    entries: List[Dict[str, Any]] = []
    i = 0
    n = len(lines)
    while i < n:
        line = lines[i].strip()
        if header_pat.match(line):
            current_object = _to_object_id_from_header(line)
            current_title = None
            current_return_desc = None
            i += 1
            continue
        m_title = title_pat.match(line)
        if m_title:
            current_title = m_title.group(1).strip()
            i += 1
            if i < n:
                m_ret = ret_pat.match(lines[i].strip())
                if m_ret:
                    current_return_desc = m_ret.group(1).strip()
                    i += 1
            continue
        m_start = method_start_pat.match(line)
        if m_start:
            method_name = m_start.group(1)
            current_entry = {
                "object": current_object or "Object",
                "title_jp": current_title or "",
                "name": method_name,
                "return_desc": current_return_desc or "",
                "return_type": _guess_return_type_from_desc(current_return_desc or ""),
                "params": [],
            }
            collecting_params = True
            i += 1
            continue
        if collecting_params and current_entry is not None:
            pm = param_pat.match(line)
            if pm:
                pname, ptype, pdesc = pm.groups()
                current_entry["params"].append(
                    {"name": pname, "type": ptype.strip(), "description": pdesc.strip()}
                )
                if closing_pat.search(line):
                    entries.append(current_entry)
                    current_entry = None
                    collecting_params = False
                i += 1
                continue
            if closing_pat.search(line):
                idx_close = line.rfind(")")
                before = line[:idx_close]
                token = before.split(",")[-1].strip()
                token = re.sub(r"[;,\s]+$", "", token)
                comment = line.split("//", 1)[1].strip() if "//" in line else ""
                synth = f"{token} // {comment}" if comment else token
                pm2 = param_pat.match(synth)
                if pm2:
                    pname, ptype, pdesc = pm2.groups()
                    current_entry["params"].append(
                        {"name": pname, "type": ptype.strip(), "description": pdesc.strip()}
                    )
                entries.append(current_entry)
                current_entry = None
                collecting_params = False
                i += 1
                continue
            i += 1
            continue
        i += 1
    return entries

def _parse_data_type_descriptions(text: str) -> Dict[str, str]:
    descriptions = {}
    current_type = None
    current_desc_lines = []
    
    normalized_text = _normalize_text(text)
    
    for line in normalized_text.split("\n"):
        line = line.strip()
        if not line:
            continue
            
        if line.startswith("â– "):
            if current_type and current_desc_lines:
                descriptions[current_type] = "\n".join(current_desc_lines).strip()
            
            current_type = line.replace("â– ", "").strip()
            current_desc_lines = []
        elif current_type:
            current_desc_lines.append(line)
            
    if current_type and current_desc_lines:
        descriptions[current_type] = "\n".join(current_desc_lines).strip()
        
    return descriptions


def extract_triples_from_specs(
    api_text: str, type_descriptions: Dict[str, str]
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    entries = _parse_api_specs(api_text)

    triples: List[Dict[str, Any]] = []
    node_props: Dict[str, Dict[str, Any]] = {}

    def _clean_type_name(type_name: str) -> str:
        return re.sub(r"\s*\(.+\)$", "", type_name).strip()

    def create_data_type_node(raw_type_name: str) -> str:
        cleaned_type_name = _clean_type_name(raw_type_name)
        if cleaned_type_name not in node_props:
            properties = {"name": cleaned_type_name}
            description = type_descriptions.get(cleaned_type_name)
            if description:
                properties["description"] = description
            node_props[cleaned_type_name] = {"type": "DataType", "properties": properties}
        return cleaned_type_name

    for e in entries:
        obj_name = e["object"] or "Object"
        method_name = e["name"]
        title_jp = e.get("title_jp", "")
        ret_desc = e.get("return_desc", "")
        ret_type_raw = e.get("return_type", "ä¸æ˜")
        params = e.get("params", [])

        node_props.setdefault(obj_name, {"type": "Object", "properties": {"name": obj_name}})
        node_props.setdefault(method_name, {"type": "Method", "properties": {"name": method_name, "description": title_jp}})
        ret_node_id = f"{method_name}_ReturnValue"
        node_props.setdefault(ret_node_id, {"type": "ReturnValue", "properties": {"description": ret_desc}})
        
        cleaned_ret_type = create_data_type_node(ret_type_raw)

        triples.append({
            "source": method_name, "source_type": "Method",
            "label": "BELONGS_TO", "target": obj_name, "target_type": "Object"
        })
        triples.append({
            "source": method_name, "source_type": "Method",
            "label": "HAS_RETURNS", "target": ret_node_id, "target_type": "ReturnValue"
        })
        triples.append({
            "source": ret_node_id, "source_type": "ReturnValue",
            "label": "HAS_TYPE", "target": cleaned_ret_type, "target_type": "DataType"
        })

        for i, p in enumerate(params):
            pname = p.get("name") or "Param"
            ptype_raw = p.get("type") or "å‹"
            pdesc = p.get("description") or ""
            param_node_id = f"{method_name}_{pname}"
            node_props.setdefault(param_node_id, {
                "type": "Parameter",
                "properties": {"name": pname, "description": pdesc, "order": i}
            })
            cleaned_ptype = create_data_type_node(ptype_raw)
            triples.append({
                "source": method_name, "source_type": "Method",
                "label": "HAS_PARAMETER", "target": param_node_id, "target_type": "Parameter"
            })
            triples.append({
                "source": param_node_id, "source_type": "Parameter",
                "label": "HAS_TYPE", "target": cleaned_ptype, "target_type": "DataType"
            })

    return triples, node_props

def _extract_method_calls_from_script(script_text: str) -> List[Dict[str, str]]:
    tree = parser.parse(bytes(script_text, "utf8"))
    root_node = tree.root_node
    calls = []
    
    def find_calls(node):
        if node.type == 'call':
            function_node = node.child_by_field_name('function')
            if function_node and function_node.type == 'attribute':
                obj_node = function_node.child_by_field_name('object')
                method_node = function_node.child_by_field_name('attribute')
                args_node = node.child_by_field_name('arguments')
                if obj_node and method_node and args_node:
                    calls.append({
                        "object_name": obj_node.text.decode('utf8'),
                        "method_name": method_node.text.decode('utf8'),
                        "arguments": args_node.text.decode('utf8'),
                        "full_text": node.text.decode('utf8'),
                    })
        for child in node.children:
            find_calls(child)

    find_calls(root_node)
    return calls

def _triples_to_graph_documents(triples: List[Dict[str, Any]], node_props: Dict[str, Dict[str, Any]]) -> List[GraphDocument]:
    node_map: Dict[str, Node] = {}
    for node_id, meta in node_props.items():
        if node_id in node_map:
            existing_node = node_map[node_id]
            existing_node.properties.update(meta.get("properties", {}))
        else:
            ntype = meta["type"]
            props = meta.get("properties", {})
            node_map[node_id] = Node(id=node_id, type=ntype, properties=props)

    rels: List[Relationship] = []
    for t in triples:
        source_node = node_map.get(t["source"])
        if not source_node:
            source_node = Node(id=t["source"], type=t["source_type"])
            node_map[t["source"]] = source_node

        target_node = node_map.get(t["target"])
        if not target_node:
            target_node = Node(id=t["target"], type=t["target_type"])
            node_map[t["target"]] = target_node

        rels.append(
            Relationship(
                source=source_node,
                target=target_node,
                type=t["label"],
                properties={}
            )
        )

    doc = Document(page_content="API Spec and Example graph")
    gdoc = GraphDocument(nodes=list(node_map.values()), relationships=rels, source=doc)
    return [gdoc]


def _rebuild_graph_in_neo4j(graph_docs: List[GraphDocument]) -> Tuple[int, int]:
    graph = Neo4jGraph(
        url=NEO4J_URI,
        username=NEO4J_USER,
        password=NEO4J_PASSWORD,
        database=NEO4J_DATABASE,
    )
    
    print("ğŸ§¹ Neo4jã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ä¸­...")
    delete_query = "MATCH (n) DETACH DELETE n"
    graph.query(delete_query)
    
    print(f"\nğŸš€ Neo4jã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ä¸­...")
    
    graph.add_graph_documents(graph_docs)
    
    res_nodes = graph.query("MATCH (n) RETURN count(n) AS c")
    res_rels = graph.query("MATCH ()-[r]->() RETURN count(r) AS c")
    return int(res_nodes[0]["c"]), int(res_rels[0]["c"])


def _build_and_load_chroma(
    api_entries: List[Dict[str, Any]], script_files: List[Tuple[str, str]]
    ) -> None:
    print("\nğŸš€ ChromaDBã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆãƒ»ä¿å­˜ä¸­...")

    if CHROMA_PERSIST_DIR.exists():
        shutil.rmtree(CHROMA_PERSIST_DIR)
    CHROMA_PERSIST_DIR.mkdir(exist_ok=True)

    docs_for_vectorstore: List[Document] = []

    for entry in api_entries:
        content_parts = [
            f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: {entry['object']}",
            f"ãƒ¡ã‚½ãƒƒãƒ‰å: {entry['name']}",
            f"èª¬æ˜: {entry['title_jp']}",
            f"è¿”ã‚Šå€¤: {entry['return_desc']}",
        ]
        if entry["params"]:
            param_texts = [
                f"- {p['name']} ({p['type']}): {p['description']}"
                for p in entry["params"]
            ]
            content_parts.append("ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\n" + "\n".join(param_texts))
        content = "\n".join(content_parts)
        metadata = {
            "source": "api_spec", "object": entry["object"], "method_name": entry["name"],
        }
        docs_for_vectorstore.append(Document(page_content=content, metadata=metadata))

    for script_name, script_content in script_files:
        content = f"ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹: {script_name}\n\n```python\n{script_content}\n```"
        metadata = {"source": "script_example", "script_name": script_name}
        docs_for_vectorstore.append(Document(page_content=content, metadata=metadata))

    html_files = _read_html_files()
    for file_name, html_content in html_files:
        soup = BeautifulSoup(html_content, 'lxml')
        content_div = soup.find('div', class_='contents')
        if content_div:
            text_content = content_div.get_text(separator='\n', strip=True)
        else:
            text_content = soup.body.get_text(separator='\n', strip=True) if soup.body else ""
        content = f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {file_name}\n\n{text_content}"
        metadata = {"source": "html_document", "file_name": file_name}
        docs_for_vectorstore.append(Document(page_content=content, metadata=metadata))

    try:
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        vectorstore = Chroma.from_documents(
            documents=docs_for_vectorstore,
            embedding=embeddings,
            persist_directory=str(CHROMA_PERSIST_DIR),
        )
        print(f"âœ” Chroma DB created and persisted with {len(docs_for_vectorstore)} documents at: {CHROMA_PERSIST_DIR}")
    except Exception as e:
        print(f"âš  Chroma DBã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

def _build_and_load_neo4j() -> None:
    # --- 1. APIä»•æ§˜æ›¸ (api.txt, api_arg.txt) ã®è§£æ ---
    print("ğŸ“„ APIä»•æ§˜æ›¸ã‚’è§£æä¸­...")
    api_text = _read_api_text()
    api_text = _normalize_text(api_text)
    api_arg_text = _read_api_arg_text()
    type_descriptions = _parse_data_type_descriptions(api_arg_text)
    spec_triples, spec_node_props = extract_triples_from_specs(api_text, type_descriptions)
    print(f"âœ” APIä»•æ§˜æ›¸ã‹ã‚‰ãƒˆãƒªãƒ—ãƒ«ã‚’æŠ½å‡º: {len(spec_triples)} ä»¶")

    # --- 2. ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ (data/*.py) ã®è§£æ ---
    print("\nğŸ ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ (data/*.py) ã‚’è§£æä¸­...")
    script_files = _read_script_files()
    if not script_files:
        print("âš  data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è§£æå¯¾è±¡ã® .py ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        script_triples, script_node_props = [], {}
    else:
        all_script_triples = []
        all_script_node_props = {}
        for script_path, script_text in script_files:
            print(f"  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­: {script_path}")
            triples, node_props = extract_triples_from_script(script_path, script_text)
            all_script_triples.extend(triples)
            all_script_node_props.update(node_props)
        script_triples = all_script_triples
        script_node_props = all_script_node_props
        print(f"âœ” ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã‹ã‚‰ãƒˆãƒªãƒ—ãƒ«ã‚’ç·è¨ˆ: {len(script_triples)} ä»¶")

    # --- 3. ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆæº–å‚™ ---
    gdocs = _triples_to_graph_documents(spec_triples + script_triples, {**spec_node_props, **script_node_props})
    
    # --- 4. éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ (HTML) ã®è§£æ ---
    print("\nğŸ“„ HTMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’LLMã§è§£æä¸­...")
    html_files = _read_html_files()
    # â˜…â˜…â˜… å¤‰æ›´ç‚¹ â˜…â˜…â˜… JSONå‡ºåŠ›ç”¨ã«HTMLã‹ã‚‰ã®æŠ½å‡ºçµæœã‚’ä¿æŒã™ã‚‹ãƒªã‚¹ãƒˆ
    serializable_html_data = []
    if not html_files:
        print("âš  data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è§£æå¯¾è±¡ã® .html ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        html_graph_docs = []
    else:
        html_graph_docs = []
        for file_name, html_content in html_files:
            print(f"  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­: {file_name}")
            try:
                graph_docs_from_html = _extract_graph_from_html(file_name, html_content)
                print(f"    - LLMãŒæŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ from '{file_name}':")
                if not graph_docs_from_html or (not graph_docs_from_html[0].nodes and not graph_docs_from_html[0].relationships):
                    print("      - ãƒ‡ãƒ¼ã‚¿ã¯æŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    # â˜…â˜…â˜… å¤‰æ›´ç‚¹ â˜…â˜…â˜… æŠ½å‡ºçµæœã‚’JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
                    for doc in graph_docs_from_html:
                        serializable_html_data.append({
                            "file_name": file_name,
                            "nodes": [node.__dict__ for node in doc.nodes],
                            "relationships": [
                                {
                                    "source": rel.source.id,
                                    "target": rel.target.id,
                                    "type": rel.type,
                                    "properties": rel.properties
                                }
                                for rel in doc.relationships
                            ]
                        })
                        if doc.nodes:
                            print("      - Nodes:")
                            for node in doc.nodes:
                                print(f"        - (ID: {node.id}, Type: {node.type}, Properties: {node.properties})")
                        if doc.relationships:
                            print("      - Relationships:")
                            for rel in doc.relationships:
                                print(f"        - ({rel.source.id}) -[:{rel.type}]-> ({rel.target.id})")
                html_graph_docs.extend(graph_docs_from_html)
            except Exception as e:
                print(f"  âš  ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"âœ” HTMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚°ãƒ©ãƒ•æƒ…å ±ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")

    # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
    # â˜…â˜…â˜… æ–°æ©Ÿèƒ½: æŠ½å‡ºã—ãŸå…¨ãƒˆãƒªãƒ—ãƒ«æƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ› â˜…â˜…â˜…
    # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
    print("\nğŸ’¾ æŠ½å‡ºã—ãŸãƒˆãƒªãƒ—ãƒ«ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ä¸­...")
    output_data = {
        "timestamp": datetime.now().isoformat(),
        "sources": {
            "api_specifications": {
                "triples": spec_triples,
                "node_properties": spec_node_props
            },
            "script_examples": {
                "triples": script_triples,
                "node_properties": script_node_props
            },
            "html_documents": serializable_html_data
        }
    }
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f"extracted_triples_{timestamp_str}.json"
    try:
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"âœ” ãƒ‡ãƒ¼ã‚¿ã‚’ '{output_filename}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âš  JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


    # --- 5. ã‚°ãƒ©ãƒ•DBã¸ã®ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ ---
    print("\nğŸ”— ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ä¸­...")
    gdocs.extend(html_graph_docs)
    try:
        node_count, rel_count = _rebuild_graph_in_neo4j(gdocs)
        print(f"âœ” ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å†æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸ: ãƒãƒ¼ãƒ‰={node_count}, ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—={rel_count}")
    except ServiceUnavailable as se:
        print(f"âš  Neo4j ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {se}")
        print("   Neo4jã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        print(f"âš  ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def main() -> None:
    _build_and_load_neo4j()

    print("\n--- ChromaDBæ§‹ç¯‰ãƒ—ãƒ­ã‚»ã‚¹ ---")
    api_text = _read_api_text()
    api_text = _normalize_text(api_text)
    api_entries = _parse_api_specs(api_text)
    print(f"âœ” {len(api_entries)}ä»¶ã®APIä»•æ§˜ã‚’è§£æã—ã¾ã—ãŸã€‚")
    script_files = _read_script_files()
    if script_files:
        print(f"âœ” {len(script_files)}ä»¶ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    else:
        print("âš  ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    _build_and_load_chroma(api_entries, script_files)

if __name__ == "__main__":
    main()